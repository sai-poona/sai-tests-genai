--- ./sample_code/variation1/src/main/java/com/example/glue.java
+++ ./sample_code/variation2/src/main/java/com/example/glue.java
@@ -1,89 +1,68 @@
-package com.example;
-
+import com.amazonaws.auth.AWSStaticCredentialsProvider;
+import com.amazonaws.auth.BasicAWSCredentials;
 import com.amazonaws.services.dynamodbv2.AmazonDynamoDB;
 import com.amazonaws.services.dynamodbv2.AmazonDynamoDBClientBuilder;
 import com.amazonaws.services.dynamodbv2.document.DynamoDB;
 import com.amazonaws.services.dynamodbv2.document.Table;
 import com.amazonaws.services.dynamodbv2.document.PutItemOutcome;
 import com.amazonaws.services.dynamodbv2.document.Item;
-import com.amazonaws.services.glue.GlueContext;
-import com.amazonaws.services.glue.util.JsonOptions;
-import com.amazonaws.services.kinesis.AmazonKinesis;
-import com.amazonaws.services.kinesis.AmazonKinesisClientBuilder;
-import com.amazonaws.services.kinesis.model.GetRecordsRequest;
-import com.amazonaws.services.kinesis.model.GetRecordsResult;
-import com.amazonaws.services.kinesis.model.GetShardIteratorRequest;
-import com.amazonaws.services.kinesis.model.GetShardIteratorResult;
-import com.amazonaws.services.kinesis.model.ShardIteratorType;
-import org.apache.spark.api.java.JavaSparkContext;
+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.KinesisClientLibConfiguration;
+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.Worker;
+import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.SparkSession;
-import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.functions;
-import scala.collection.JavaConverters;
+import java.util.UUID;
 
-import java.util.List;
-import java.util.Map;
+public class GlueETLJob {
+    public static void main(String[] args) {
+        // Initialize Spark Session
+        SparkSession spark = SparkSession.builder()
+                .appName("GlueETLJob")
+                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
+                .getOrCreate();
 
-public class GlueJob {
+        // Kinesis Stream Config
+        String kinesisStreamName = "your-kinesis-stream-name";
+        String kinesisRegion = "your-region";
+        String dynamoDBTableName = "your-dynamodb-table-name";
 
-    public static void main(String[] args) {
-        // Initialize Spark session and Glue context
-        SparkSession spark = SparkSession.builder().appName("GlueJob").getOrCreate();
-        JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());
-        GlueContext glueContext = new GlueContext(jsc);
+        // Initialize Kinesis Stream Reader
+        Dataset<Row> kinesisData = spark.readStream()
+                .format("kinesis")
+                .option("streamName", kinesisStreamName)
+                .option("region", kinesisRegion)
+                .option("initialPosition", "earliest")
+                .load();
 
-        // Kinesis stream configuration
-        String kinesisStreamName = "your-kinesis-stream-name";
-        String regionName = "your-region";
-        
-        // DynamoDB table configuration
-        String dynamodbTableName = "your-dynamodb-table-name";
-        String dynamodbRegionName = "your-region";
-
-        // Create Kinesis client
-        AmazonKinesis kinesisClient = AmazonKinesisClientBuilder.standard()
-                .withRegion(regionName)
-                .build();
-
-        // Get shard iterator
-        GetShardIteratorRequest shardIteratorRequest = new GetShardIteratorRequest()
-                .withStreamName(kinesisStreamName)
-                .withShardId("shardId-000000000000")
-                .withShardIteratorType(ShardIteratorType.LATEST);
-        GetShardIteratorResult shardIteratorResult = kinesisClient.getShardIterator(shardIteratorRequest);
-        String shardIterator = shardIteratorResult.getShardIterator();
-
-        // Get records from Kinesis
-        GetRecordsRequest recordsRequest = new GetRecordsRequest().withShardIterator(shardIterator);
-        GetRecordsResult recordsResult = kinesisClient.getRecords(recordsRequest);
-        List<Record> records = recordsResult.getRecords();
-
-        // Process records with Spark
-        List<String> jsonRecords = records.stream()
-                .map(record -> new String(record.getData().array()))
-                .collect(Collectors.toList());
-
-        Dataset<Row> dataFrame = spark.read().json(jsc.parallelize(jsonRecords));
-        
-        // Example transformation
-        Dataset<Row> transformedDataFrame = dataFrame.filter(dataFrame.col("column").equalTo("value"));
+        // Perform Transformation
+        Dataset<Row> transformedData = kinesisData
+                .withColumn("new_column", functions.lit("transformed"));
 
         // Write to DynamoDB
-        AmazonDynamoDB dynamoDBClient = AmazonDynamoDBClientBuilder.standard()
-                .withRegion(dynamodbRegionName)
-                .build();
-        DynamoDB dynamoDB = new DynamoDB(dynamoDBClient);
-        Table table = dynamoDB.getTable(dynamodbTableName);
+        transformedData.foreachPartition(partition -> {
+            // DynamoDB Client Initialization
+            AmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard()
+                    .withRegion(kinesisRegion)
+                    .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials("your-access-key", "your-secret-key")))
+                    .build();
+            DynamoDB dynamoDB = new DynamoDB(client);
+            Table table = dynamoDB.getTable(dynamoDBTableName);
 
-        transformedDataFrame.foreach(row -> {
-            Map<String, Object> map = row.getValuesMap(row.schema().fieldNames());
-            Item item = new Item();
-            map.forEach(item::with);
-            PutItemOutcome outcome = table.putItem(item);
+            partition.forEachRemaining(row -> {
+                // Convert Spark Row to DynamoDB Item
+                Item item = new Item()
+                        .withPrimaryKey("id", UUID.randomUUID().toString())
+                        .withString("new_column", row.getString(row.fieldIndex("new_column")));
+                PutItemOutcome outcome = table.putItem(item);
+            });
         });
 
-        // Stop Spark session
-        spark.stop();
+        // Start the Spark Stream
+        try {
+            spark.streams().awaitAnyTermination();
+        } catch (StreamingQueryException e) {
+            e.printStackTrace();
+        }
     }
 }
